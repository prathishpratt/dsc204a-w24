---
title: Week 10
class: DSC204A
status: Active
---

Mar 11
: **1**{: .label} ML System - 2
  : [Slides](assets/slides/22_ml-system-2.pdf) &#8226; [Recording](https://podcast.ucsd.edu/watch/wi24/dsc204a_a00/26) &#8226; [Scribe Notes](assets/scribe_notes/Mar_11_scribe_note.pdf)
: *Reading:*
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)




Mar 13
: **2**{: .label} ML System - 3
  : [Slides](assets/slides/23_ml-system-3.pdf) &#8226; [Recording](https://podcast.ucsd.edu/watch/wi24/dsc204a_a00/27) &#8226; [Scribe Notes](#)
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)




Mar 15
: **3**{: .label} ML System - 4
  : [Slides](assets/slides/24_ml-system-4.pdf) &#8226; [Recording](https://podcast.ucsd.edu/watch/wi24/dsc204a_a00/28) &#8226; [Scribe Notes](#)
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)

